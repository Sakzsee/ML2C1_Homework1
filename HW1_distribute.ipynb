{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distribute_HW1",
      "provenance": [],
      "collapsed_sections": [
        "qkRlyqZTz4CJ",
        "X0xOsPAh0NI7",
        "K8Bnn7qe1ESn",
        "VOJ6cAUzzIyX",
        "w37B2HifeZjm",
        "N_sIHuKbmLfi",
        "6QZV2RGN6ZB8",
        "TPNUF_RFBDK3",
        "UQPS4R_XFgmj",
        "pc9odd-eLzBm",
        "4fTrnp7zQMsq",
        "dPbpzXXmh7R2",
        "-5_BDEbNMR5a",
        "xEZXt6t7ZsOj",
        "GnEmcDVw7ExX",
        "6xULnXHxmtOf",
        "B363lqGF4ZsX",
        "F_HB3eZsGrB8",
        "AFsD6kCSH_wH",
        "fESTBzaYOTFh"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaA1H2AHyVWQ"
      },
      "source": [
        "# ML-2: Trees, Model Interrogation and Bayesian Workflow\n",
        "# Homework 1: Harnessing data to help the heart!\n",
        "\n",
        "**ML-2 Cohort 1** <br>\n",
        "**Instructor: Dr. Rahul Dave**<br>\n",
        "**Max Score: 100** <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkRlyqZTz4CJ"
      },
      "source": [
        "#### **Name of people who have worked on this homework:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQSvK4tZ3ias"
      },
      "source": [
        "## Table of Contents "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llCqCNUTz7wG"
      },
      "source": [
        "* HW-1: Harnessing data to help the heart!\n",
        "  * Instructions\n",
        "  * Learning Goals\n",
        "  * Loading the DataFrame\n",
        "  * Q1: Exploratory Data Analysis (EDA and Data Pre-Preprocessing) **(10 marks)**\n",
        "    * 1.1 Understanding the data\n",
        "    * 1.2 Data Pre-processing\n",
        "      * 1.2.1 Split the data\n",
        "      * 1.2.2 Combine the X_train and y_train into one dataframe\n",
        "    * 1.3 Exploratory Data Analysis \n",
        "      * 1.3.1 Show the target distribution? - Number of people with no Heart Disease and Number of people with Heart Disease\n",
        "      * 1.3.2 How is age distributed? What is the most common age to get a heart disease? How is gender related to age?\n",
        "      * 1.3.3 Which has the most postive and negative correlation with the target variable?\n",
        "      * 1.3.4 Correlation between Blood Sugar level, Slope type and Thalessemia type with target\n",
        "      * 1.3.5 Plot the histogram for the all variables in train set\n",
        "    * 1.4 Defining Accuracy function\n",
        "  * Q2: Models **(80 marks)**\n",
        "    * 2.1 Baseline model - Logistic Regression \n",
        "    * 2.2 Decision Trees \n",
        "      * 2.2.1 Pruning \n",
        "        * 2.2.1.1 Pruning based on cost complexity parameter\n",
        "        * 2.2.1.2 Pruning based on max depth \n",
        "      * 2.2.2 Comparing Decision Tree Models \n",
        "    * 2.3 Bagging Tree Model \n",
        "    * 2.4 Random Forest \n",
        "      * 2.4.1 Permutance Importance \n",
        "      * 2.4.2 Out of Bag Score \n",
        "    * 2.5 Boosting \n",
        "    * 2.6 XGBoost \n",
        "  * Q3: Comparing the Models**(10 marks)**\n",
        "  * Bonus Question **(20 marks)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxkV2910pFEw"
      },
      "source": [
        "## Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhwik43rpHj_"
      },
      "source": [
        "- This homework should be submitted in pairs.\n",
        "\n",
        "- Ensure you and your partner together have submitted the homework only once. Multiple submissions of the same work will be penalised and will cost you 2 points.\n",
        "\n",
        "- Please restart the kernel and run the entire notebook again before you submit.\n",
        "\n",
        "- Running cells out of order is a common pitfall in Notebooks. To make sure your code works restart the kernel and run the whole notebook again before you submit. \n",
        "\n",
        "- To work on the homework, you will first need to fork the repository into your GitHub account and clone it to work on it on your local computer. To submit your homework, push your homework into the same GitHub and upload the link on edStem.\n",
        "\n",
        "- Submit the homework well before the given deadline. Submissions after the deadline will not be graded.\n",
        "\n",
        "- We have tried to include all the libraries you may need to do the assignment in the imports statement at the top of this notebook. We strongly suggest that you use those and not others as we may not be familiar with them.\n",
        "\n",
        "- Comment your code well. This would help the graders in case there is any issue with the notebook while running. It is important to remember that the graders will not troubleshoot your code. \n",
        "\n",
        "- Please use .head() when viewing data. Do not submit a notebook that is **excessively long**. \n",
        "\n",
        "- In questions that require code to answer, such as \"calculate the $R^2$\", do not just output the value from a cell. Write a `print()` function that includes a reference to the calculated value, **not hardcoded**. For example: \n",
        "```\n",
        "print(f'The R^2 is {R:.4f}')\n",
        "```\n",
        "- Your plots should include clear labels for the $x$ and $y$ axes as well as a descriptive title (\"MSE plot\" is not a descriptive title; \"95 % confidence interval of coefficients of polynomial degree 5\" is).\n",
        "\n",
        "- **Ensure you make appropriate plots for all the questions it is applicable to, regardless of it being explicitly asked for.**\n",
        "\n",
        "<hr style=\"height:2pt\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0xOsPAh0NI7"
      },
      "source": [
        "## Learning Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTWGfd3i0QI6"
      },
      "source": [
        "We will look here into the practicalities of fitting classification trees, random forests, and boosted trees. These involve out-of-bound estimates and cross-validation, and how you might want to deal with hyperparameters in these models. \n",
        "\n",
        "The homework is divided into four main parts:\n",
        "1. Exploratory Data Analysis on the dataset and data-preprocessing\n",
        "2. Developing Baseline model using Logistic Regression\n",
        "3. Working with different tree models\n",
        "4. Comparing the models to check which performed better"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dXXAM_r03B_"
      },
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
        "from matplotlib import colors\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import tree\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8Bnn7qe1ESn"
      },
      "source": [
        "## Load the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHb-FlYv1IVI"
      },
      "source": [
        "#import the dataset\n",
        "df = pd.read_csv('heart_data.csv') \n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOJ6cAUzzIyX"
      },
      "source": [
        "## Part 1: Exploratory Data Analysis and Data-Preprocessing **(10 marks)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w37B2HifeZjm"
      },
      "source": [
        "### 1.1 Understanding the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5RWtWp7zXnV"
      },
      "source": [
        "**The data consist 303 individual data with 13 features and output label. Features are as follows**\n",
        "\n",
        "* **Age** : displays the age of the individual.\n",
        "* **Sex** : displays the gender of the individual using the following format:\n",
        "    * 1 = male \n",
        "    * 0 = female\n",
        "* **Chest-pain type (cp)**: displays the type of chest-pain experienced by the individual using the following format : \n",
        "    * 1 = typical angina \n",
        "    * 2 = atypical angina \n",
        "    * 3 = non — anginal pain \n",
        "    * 4 = asymptotic\n",
        "* **Resting Blood Pressure (trestbps)**: displays the resting blood pressure value of an individual in mmHg (unit)\n",
        "* **Serum Cholesterol (chol)**: displays the serum cholesterol in mg/dl (unit)\n",
        "* **Fasting Blood Sugar (fbs)**: compares the fasting blood sugar value of an individual with 120mg/dl. \n",
        "  * If fasting blood sugar > 120mg/dl then : 1 (true) \n",
        "  * If fasting blood sugar < 120mg/dl then : 0 (false)\n",
        "* **Resting ECG (restecg)**: displays resting electrocardiographic results \n",
        "  * 0 = normal\n",
        "  * 1 = having ST-T wave abnormality \n",
        "  * 2 = left ventricular hypertrophy\n",
        "*  **Max heart rate achieved (thalach)**: displays the max heart rate achieved by an individual.\n",
        "* **Exercise induced angina (exang)**: \n",
        "  * 1 = yes \n",
        "  * 0 = no\n",
        "* **ST depression induced by exercise relative to rest (oldpeak)**: displays the value which is integer or float.\n",
        "* **Peak exercise ST segment (slope)**: \n",
        "  * 1 = upsloping \n",
        "  * 2 = flat \n",
        "  * 3 = downsloping\n",
        "* **Number of major vessels (0–3) colored by fluoroscopy (ca)**: displays the value as an integer or float.\n",
        "* **Thal** : displays the thalassemia : \n",
        "  * 1 = normal \n",
        "  * 2 = fixed defect \n",
        "  * 3 = reversible defect\n",
        "* **(Output)Diagnosis of heart disease (target)**: Displays whether the individual is suffering from heart disease or not : \n",
        "  * 0 = absence \n",
        "  * 1 = presence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8lat3Q40A64"
      },
      "source": [
        "#check that dataset datatypes and check nulls \n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_sIHuKbmLfi"
      },
      "source": [
        "### 1.2 Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLT4c8dCrFGh"
      },
      "source": [
        "#set the random state as 42 throughout the homework\n",
        "rf=42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joeUEaVG2F9i"
      },
      "source": [
        "#### 1.2.1 Splitting the data\n",
        "1. First split the data into input and output variables \n",
        "2. Then split the data into train and test, with **0.3 as the test size** and random state is set as rf(as defined previously)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq-rCJg_-1bu"
      },
      "source": [
        "#splitting the target and input variables\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AgDDZTwApSO"
      },
      "source": [
        "#train test split - split with 0.3 as test size and random state is rf\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT3X6jFret28"
      },
      "source": [
        "#### 1.2.2 For us to do EDA on the train set, we will need to combine the X_train and y_train into one dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp_lImBbMhsB"
      },
      "source": [
        "#combining X_train and y_train into a single dataframe\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QZV2RGN6ZB8"
      },
      "source": [
        "### 1.3 Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsgvjt95582f"
      },
      "source": [
        "**The most efficient way to interpret data and to find relations between the different variables is by Visualisation.**\n",
        " \n",
        "**Hence answer the following questions using appropriate visualisation tool(plotting).** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfGkTclT6DFe"
      },
      "source": [
        "#### 1.3.1 Show the target distribution? - Number of people with no Heart Disease and Number of people with Heart Disease"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXy1d5K7501d"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn-pKWEP6R5U"
      },
      "source": [
        "#### 1.3.2 How is age distributed? What is the most common age to get a heart disease? How is gender related to age?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvRae2fa6hXX"
      },
      "source": [
        "#checking the distribution of age\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRd5AfFQc45y"
      },
      "source": [
        "#plot to check variation of age wrt to target \n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV45RQg8hy5o"
      },
      "source": [
        "#age vs gender\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f1gfrz9hCTQ"
      },
      "source": [
        "your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baTxzMaRjP71"
      },
      "source": [
        "#### 1.3.3 Which has the most postive and negative correlation with the target variable?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzdUKArvjGV4"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYTJgGjrjV8u"
      },
      "source": [
        "your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPysFunxkCe6"
      },
      "source": [
        "#### 1.3.4 Correlation between Blood Sugar level, Slope type and Thalessemia type with target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bM8hzi3kQ5s"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xzinIUFkpUQ"
      },
      "source": [
        "your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hplCr5PRnD9u"
      },
      "source": [
        "#### 1.3.5 Plot the histogram for the all variables in train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPOIPNqpnacA"
      },
      "source": [
        "#plot histogram for all variables\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VfA1kjlmhhD"
      },
      "source": [
        "##### Based on the histogram plotted above, it looks like we need to standardise the data, right? But tree models do not need standardising, explain why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfEyENobKWWU"
      },
      "source": [
        "your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPNUF_RFBDK3"
      },
      "source": [
        "### 1.4 Defining function to calculate accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-suY4EaMKcmH"
      },
      "source": [
        "We will define a function to calculate the model accuracy using `accuracy_score`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh_M6gLqAx3h"
      },
      "source": [
        "#Function to get test accuracy\n",
        "def accuracy(X, y, model):\n",
        "    #X is the testing data to predict on, y is the actual data\n",
        "    #your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHgF_9wLLvEs"
      },
      "source": [
        "## Part 2: Models! **(80 marks)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcrqR8XgKx1u"
      },
      "source": [
        "**Now that we have completed understanding the data and pre-processing the data, lets move on to the next step of building models on the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJay44vtciQZ"
      },
      "source": [
        "We will use two metrics to evaluate our model.\n",
        "1. The accuracy function we defined - this will help us understand the training and testing accuracy of the model\n",
        "2. Confusion matrix to evaluate the model\n",
        "\n",
        "Remember the basics of Confusion Matrix: \n",
        "\n",
        "![Cm.jpeg](https://drive.google.com/uc?export=view&id=14q4KVVqWiP6C0dve4745UX7LN3vKl0i6)\n",
        "\n",
        "\n",
        "This is how sklearn plots it confusion matrix, hence remember this in each step where you are required to do analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQPS4R_XFgmj"
      },
      "source": [
        "### 2.1 Baseline Model - Logistic Regression **(5 marks)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTF9FKnsXn6r"
      },
      "source": [
        "We will start with the most basic classification algorithm - **Logistic Regression**. \n",
        "\n",
        "1. We will create a pipeline - where we will pass the `('standardizer', StandardScaler()` and sklearn's logistic regression with random_state as rf(42). Lets call this model **LR_model**\n",
        "2. Create the parameter_grid to be passed to the model - this can be anything, you are cross validating to get the best score for your model, hence chose the parameters you want to define. \n",
        "3. Pass the pipeline and the parameter_grid to the GridSearchCV, with `cv =10 ` and `scoring = \"accuracy\"`\n",
        "4. Fit the model\n",
        "5. Print the performance of the model(training and testing accuracy) using the accuracy function defined by you. \n",
        "7. Print out the best params of the model\n",
        "6. Plot the confusion matrix to evaluate the model. \n",
        "4. Based on the confusion matrix and the accuracy - **draw conclusions about the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qivmsgNUSbLB"
      },
      "source": [
        "#build a pipeline\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owdXuIqv3OpF"
      },
      "source": [
        "#hyper-parameters tuning \n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvbpCTMrS2Ta"
      },
      "source": [
        "#fit the model\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ynDqJoCB02G"
      },
      "source": [
        "#check the performance- accuracy\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo60EPBW4PdL"
      },
      "source": [
        "#get the best parameters\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c95cy0yQLVHT"
      },
      "source": [
        "#plot the confusion matrix\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OgMNDw3sItB"
      },
      "source": [
        "#### What can you say about the model? By looking the the accuracy and the confusion matrix. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3B4MB7vsPbJ"
      },
      "source": [
        "your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc9odd-eLzBm"
      },
      "source": [
        "### 2.2 Decision Tree **(30 marks)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWuX0RcHOZa0"
      },
      "source": [
        "We will use a simple Decision Tree Classifer to classify the target variable. \n",
        "This is what you ought to keep in mind about decision trees.\n",
        "\n",
        "**from the docs of sklearn:**\n",
        "\n",
        "* `max_depth` : int or None, optional (default=None)\n",
        "  * The maximum depth of the tree. If **None**, then nodes are expanded until all leaves are pure or until all leaves contain less than `min_samples_split` samples.\n",
        "* `min_samples_split` : int, float, optional (default=2)\n",
        "\n",
        "**Remember:**\n",
        "\n",
        "The deeper the tree, the more prone you are to overfitting.\n",
        "The smaller `min_samples_split`, the more the overfitting. One may use `min_samples_leaf` instead. More samples per leaf, the higher the bias.\n",
        "\n",
        "**Steps to follow:**\n",
        "\n",
        "\n",
        "1. Use sklearn's DecisionTreeClassifer to fit the model on the training set - **do not** use `max_depth` here - let it be **default(None)** - we want to see how much the depth goes upto, and set the `random_state = rf`. Call this model **DecisionTree**\n",
        "2. Print the performance of the model(training and testing accuracy) using the accuracy function defined by you. \n",
        "3. Plot the confusion matrix to evaluate the model.\n",
        "4. Plot the Decision Tree built using sklearn's tree \n",
        "5. Based on the confusion matrix and the accuracy - draw conclusions about the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggITbsSHLjCg"
      },
      "source": [
        "#fit the model\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viy2f2A-L3AK"
      },
      "source": [
        "#Performance\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_TXEg7jMAg_"
      },
      "source": [
        "#plot the tree\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoqjtK3AL8E5"
      },
      "source": [
        "#plot the confusion matrix\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fTrnp7zQMsq"
      },
      "source": [
        "#### What conclusions can you draw from the confusion matrix above? How different is this from the Logistic Regression Model defined by you \n",
        " \n",
        "your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPbpzXXmh7R2"
      },
      "source": [
        "### 2.2.1 Pruning **(25 marks)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FITD9M-zvA6x"
      },
      "source": [
        "Now you see based on the accuracy and the confusion matrix, your model is overfitting. Overfitting in Decision Trees is extremely common. To solve this issue, we will opt for two different approaches:\n",
        "\n",
        "1. **Pruning** by choosing the best cost complexity parameter(`ccp_alpha`), `ccp_alpha` controls how less or more the pruning happens. \n",
        "  * Greater values of `ccp_alpha` increase the number of nodes pruned. - A way to find the best `ccp_alpha` value is by plotting the testing and training accuracy for all `ccp_alpha` values, and then cross validating\n",
        "\n",
        "2. By finding the best **max_depth**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZurAhjHiDi_"
      },
      "source": [
        "#### 2.2.1.1 Pruning using Cost complexity parameter - ccp_alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_BDEbNMR5a"
      },
      "source": [
        "##### Cost Complexity Pruning: Visualize alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg23ISN4v7H2"
      },
      "source": [
        "For finding the `best alpha` follow the steps below:\n",
        "\n",
        "1. From the previous Decision Tree built, get all the values of `ccp_alphas` and build a pruned tree for **each value** of `ccp_alphas`. \n",
        "\n",
        "2. Plot the alpha values with respect to the training and testing accuracy. You plot should look something like this(**remember your plot will vary based on your model hence it will not look exactly like this**)\n",
        "![accuracy.jpeg](https://drive.google.com/uc?export=view&id=1aUAAOS5rvsXQpadMNhDlbinpA7OJCais)\n",
        "  * Create a datadrame with the training accuracy, testing accuracy and the respective alpha values. (call this dataframe as **scores**)\n",
        "  * An example of this is below:\n",
        "  \n",
        "![ttsplit.jpeg](https://drive.google.com/uc?export=view&id=1yZnIefjMrDrSptYhysiwkRwvoRKNNUp8)\n",
        "  * From the graph find out the value of best alpha.\n",
        "  * Use this alpha to create a new model - call this model **DT_test**. \n",
        "  * Check the accuracy, and the plot the confusion matrix and the tree\n",
        "  * Is the model performing better than the base Decision tree model you built?\n",
        "\n",
        "3. Now, we got the best alpha for the training set we gave initially, however there is a possibility that if we have different training sets, the accuracy of the model and the best alpha value will also vary. \n",
        "  * To check this, perform **5-fold cross validation** with training set. This will help us see if the **accuracy varies for different values of training set**. If yes, then we can move forward with find different alpha values to get the most optimal alpha\n",
        "\n",
        "4. Once you have confirmed that you get different accuracies, we will find a better alpha value than we did in the DT_test case. \n",
        "  * For this we will first check the range of alphas we got and the difference between two alpha values. We will cross validate our model using GridSearchCv - where the param_grid passed to it, will have the alpha values for the range we found and with the step_size as the difference. \n",
        "  * For example:\n",
        "  Consider the dataframe image(scores), the range of alpha is from 0 to 0.0044 with a difference between two alphas as 0.001. This is what will be passed to the param_grid. This allows us to find a better value of alpha than the previous DT_test model. Name this alpha as `ideal_alpha_value`\n",
        "4. Use this `ideal_alpha_value` in the new DecisionTreeClassifer Model - call it **DT_pruned**. \n",
        "  * Calculate the perfomance of the model\n",
        "  * Plot the confusion matrix \n",
        "  * Plot the tree\n",
        "  * Is this tree performing better than DT_test and Decision tree baseline model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvpe_w1FXWxr"
      },
      "source": [
        "##### **Step 1:** Get all the values of Alpha and build a pruned tree for each value. \n",
        "First step, is to extract all the alpha and impurity values for the previous tree(Decision_tree) that we built and build a **pruned tree** for each value of alpha. \n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "**Omit the maximum value of alpha - this is beacause the maximum value would prune all leaves, hence leaving us with only root instead of the tree.**\n",
        "\n",
        "Hence: \n",
        "\n",
        "`ccp_alphas = ccp_alphas[:-1]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG0yVvT90ivY"
      },
      "source": [
        "#Cost Complexity Pruning\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDBvGYgo0kIk"
      },
      "source": [
        "#create a dictionary to store all the alpha values, and fit a model on each value of ccp_alphas we got from the previous step\n",
        "DT_alphas = []\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pnv86BEV7_n"
      },
      "source": [
        "##### **Step 2**: Plot the training and testing accuracies, and store the values in a data frame called `scores`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAWXCxu_MXSn"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEZXt6t7ZsOj"
      },
      "source": [
        "###### **Which is the best value of alpha? and why?**\n",
        "\n",
        "your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb-SGtZC6LjA"
      },
      "source": [
        "###### Lets fit a model on the best_alpha value we obtained from the previous step, this will show us how much is the model pruned.\n",
        "\n",
        "Call this model **DT_test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXZLua-F0Boi"
      },
      "source": [
        "#fit the model with the best ccp_alpha value you got\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c57q0C1z0kr"
      },
      "source": [
        "#plot the tree to see the pruning\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww7k08DN6wxE"
      },
      "source": [
        "#check the training and testing score as well\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAvRz_j_6-Ay"
      },
      "source": [
        "#plot the confusion matrix\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnEmcDVw7ExX"
      },
      "source": [
        "###### **What do you notice here?**\n",
        "You see that the model performs slightly better,and is able to predict the true positives and false positives a little better than the base Decision tree model. \n",
        "\n",
        "We need to still get a much more accurate value of ccp_alpha to see a better pruned model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sLJbr_ZMssg"
      },
      "source": [
        "##### Step 3: Cross Validate to find the **Best Alpha**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsmh-7fzaMeu"
      },
      "source": [
        "Now that you know your best alpha. Can it be possible for a different training set your value of best alpha and accuracy can change? Lets check this!\n",
        "\n",
        "To do this we will use Cross validation, we will do 5-fold cross validation here. \n",
        "\n",
        "What we want to see here is, how accurate the tree is with different sets of training data, **while the alpha value remains the same**(i.e. Set the value of ccp_alpha same as the best value of alpha you got from the previous step)\n",
        "\n",
        "Plot tree vs accuracy. It should look something like this:\n",
        "\n",
        "![tree_accuracy.jpeg](https://drive.google.com/uc?export=view&id=15-u3_4Bp3piEFVmn8y6-wxCdKpuEHuTt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS7sE0Bdz20L"
      },
      "source": [
        "#use 5-fold cross validation to change the training set, plot the change in accuracy for each cross validated tree\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKvObMq8WNv3"
      },
      "source": [
        "Hence this proves that we get different accuracies when we do cross validation, hence this also means we wil have different values of best alphas\n",
        "\n",
        "Now lets cross validate to find the best alpha\n",
        "* So from the dataframe we built to find the best alpha, find the **range of alpha** and the **step_size**. \n",
        "* Perform GridSearchCV for the range of alphas with step size. And from this we will find the best alpha value. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF4lt6Zq_pbS"
      },
      "source": [
        "#range of alphas\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmak-QnQ_bTX"
      },
      "source": [
        "#hyper-parameter tuning and model fit\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6EiTB39AcUS"
      },
      "source": [
        "#find the best parameter\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4xRrmFqdgdM"
      },
      "source": [
        "What value of alpha is provided by the gridsearchcv?\n",
        "\n",
        "your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkem9OuseDBS"
      },
      "source": [
        "Now lets get the `ideal_ccp_alpha` from the gridsearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlp1WJYaeHS8"
      },
      "source": [
        "ideal_ccp_alpha = ___________"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufc8mvpIl2CB"
      },
      "source": [
        "##### Step 4: Fit a new Decision Tree called **DT_pruned**\n",
        "\n",
        "1. Fit a new Decision_Tree on the ideal_ccp_alpha value we got from the previous step, set the `random state = rf`\n",
        "2. Print the performance of the model(training and testing accuracy) using the accuracy function defined by you. \n",
        "3. Plot the confusion matrix to evaluate the model. \n",
        "4. Based on the confusion matrix and the accuracy - draw conclusions about the model\n",
        "5. Plot the Decision Tree built using sklearn's tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e3yEBYANJ--"
      },
      "source": [
        "#fit the model\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOUJokG8ylGv"
      },
      "source": [
        "#plot the tree\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5akz6ixNLwt"
      },
      "source": [
        "#plot the confusion matrix\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtoUuq-iypEk"
      },
      "source": [
        "#Performance\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xULnXHxmtOf"
      },
      "source": [
        "###### What conclusions can you draw from the confusion matrix above? How different is this from the Basic Decision Tree Model defined by you\n",
        "\n",
        "your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_lPN9Fs1uvd"
      },
      "source": [
        "#### 2.2.1.2 Pruning Single Decision Tree by Depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t964PlVHF0Rg"
      },
      "source": [
        "We fit here a single tree to our dataset and perform 5-fold cross validation on the training set. For **EACH depth of the tree, we fit a tree** and then compute the 5-fold CV scores. These scores are then averaged and compared across different depths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikW5g2yWjTKH"
      },
      "source": [
        "1. Write a code to cross validate for different depths( 5 fold cross validation) on the training set. Consider the value of `min_samples_split=10`\n",
        "\n",
        "2. Plot the cross validation accuracy against each depth. \n",
        "\n",
        "3. Based on the graph, chose the `best_depth`\n",
        "\n",
        "4. Using the value of `best_depth`, fit a new decision tree classifer, and check its training and testing accuracy and plot its confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csDaZy_D1rhu"
      },
      "source": [
        "#Find optimal depth of trees\n",
        "depth, tree_start, tree_end = _________\n",
        "for i in range(tree_start, tree_end):\n",
        "    #your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO5dB0WTnbAc"
      },
      "source": [
        "#Plot the variation of cross validation accuracy wrt max_depth\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYk9AgMcHPIo"
      },
      "source": [
        "##### What is the optimal depth according to you from the graph above. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHeukSwrHLQf"
      },
      "source": [
        "#Make 'best_depth' a variable\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlJh-0zbn4mn"
      },
      "source": [
        "##### Set the max_depth as best_depth obtained in previous part\n",
        "\n",
        "1. Set the max_depth as best_depth obtained in previous part, and fit the model, set the `random_state = rf`. Call this model **DT_max_depth**\n",
        "2. Print the performance of the model(training and testing accuracy) using the accuracy function defined by you. \n",
        "3. Plot the confusion matrix to evaluate the model. \n",
        "4. Based on the confusion matrix and the accuracy \n",
        "5. Plot the Decision Tree built using sklearn's tree\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkW3j2H7HV4k"
      },
      "source": [
        "#fit the model\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJNJsHTKHgYD"
      },
      "source": [
        "#Performance\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQaNdTCGHsbf"
      },
      "source": [
        "#confusion matrix\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n60bAci2BWm"
      },
      "source": [
        "#plot the tree\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj7vjg2q0cq1"
      },
      "source": [
        "#### 2.2.2 Comparing Decision Tree models\n",
        "\n",
        "Of the three Models built - Decision tree, DT_pruned(with idea_ccp_alpha) and DT_max_depth(with best_depth) which Model performs better and why?\n",
        "\n",
        "your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B363lqGF4ZsX"
      },
      "source": [
        "### 2.3 Bagging Tree Model **(10 marks)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V79mC9eaOPVU"
      },
      "source": [
        "Whats the basic idea?\n",
        "\n",
        "A Single Decision tree is likely to overfit as seen previously.\n",
        "So lets introduce replication through Bootstrap sampling.\n",
        "Bagging uses bootstrap resampling to create different training datasets. This way each training will give us a different tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvirO6Nc4dAr"
      },
      "source": [
        "\n",
        "1. Provide the `model` as baggingclassifer with `random state = rf`\n",
        "2. Set the `bagging parameters` - `n_estimators` to be a range from 30 to 150.\n",
        "3. Pass the `bagging parameters` and `model` to the `GridSearchCV` and set the `cv = 10` and `scoring = 'accuracy'`\n",
        "4. Fit the model\n",
        "5. Find the testing and training accuracy\n",
        "6. Plot the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOUU-gpo4kPN"
      },
      "source": [
        "#Hyper-parameter tuning\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxRrqp1v4mR3"
      },
      "source": [
        "#Print the best parameters\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSOjRRgs4ofq"
      },
      "source": [
        "#Performance\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eUUBrEn4qFn"
      },
      "source": [
        "#plot confusion matrix\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4Gu1uUHPL_8"
      },
      "source": [
        "Does this perform better than Decision Tree?\n",
        "\n",
        "your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVpkgV6a5CLV"
      },
      "source": [
        "### 2.4 Random Forest **(15 marks)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJA-i6WeOvDY"
      },
      "source": [
        "What's the basic idea?\n",
        "\n",
        "Bagging alone is not enough randomization, because even after bootstrapping, we are mainly training on the same data points using the same variables, and will retain much of the overfitting.\n",
        "\n",
        "So we will build each tree by splitting on **\"random\"** subset of predictors at each split (hence, each is a 'random tree'). This can't be done in with just one predictor, but with more predictors we can choose what predictors to split on randomly and how many to do this on. Then we combine many 'random trees' together by averaging their predictions, and this gets us a forest of random trees: a random forest.\n",
        "\n",
        "Below we create a hyper-param Grid(GridSearchCV). Things to remember:\n",
        "\n",
        "* `max_features` : int, float, string or None, optional (default=”auto”)\n",
        "  * The number of features to consider when looking for the best split. \n",
        "\n",
        "Steps to follow\n",
        "1. Provide the `model` as `RandomForestclassifer` with `random state = rf`\n",
        "2. Set the Rf parameters - `n_estimators`, `max_depth`, `max_features` and `min_samples_split`. - Chose the range to be what you think is suitable.\n",
        "3. Pass the Rf parameters and model to the GridSearchCV and set the `cv = 10` and `scoring = 'accuracy'`. Call this model RF_model\n",
        "4. Fit the model\n",
        "5. Find the testing and training accuracy\n",
        "6. Plot the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62qlbZdWAl90"
      },
      "source": [
        "#define the model, param_grid and fit the model\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qiq3feNv5IlN"
      },
      "source": [
        "#Print the best parameters\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL3O_IJs5KXR"
      },
      "source": [
        "#Performance\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6futWe-N5MW0"
      },
      "source": [
        "#plot the confusion matrix\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQwaiVHR5TW5"
      },
      "source": [
        "#### 2.4.1 Permutance Importance \n",
        "\n",
        "Comparison between Bagging model and Random Forest Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NonBngZJlfxu"
      },
      "source": [
        "1. Define a function to plot the permutance importance.\n",
        "2. Next call the sklearn function permutance importance and pass the Random Forest and Bagging model to it.\n",
        "3. Using the plotting function defined in step 1, compare the permutance importance plots for both the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5aCDcOQLJ-q"
      },
      "source": [
        "def p_importance(model, cols, fi, fistd = 0):\n",
        "    return pd.DataFrame({'features':cols, 'importance':fi, 'importance_std': fistd}\n",
        "                       ).sort_values('importance', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjP9F4pKLIyZ"
      },
      "source": [
        "# find permutation importance for Bagging and Random Forest models\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBKBxFXYLF1U"
      },
      "source": [
        "#plot the permutance importance using the function define above\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh8t6QPCHl-l"
      },
      "source": [
        "#### 2.4.2 Out of Bag Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUU-rywbsaZa"
      },
      "source": [
        "Out of bag (OOB) score is a way of validating the Random forest model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9mUxz0-PA2U"
      },
      "source": [
        "We'll now engage in our very own version of a grid-search, done over the out-of-bag scores that sklearn gives us for free. \n",
        "\n",
        "1. Set the values for `n_estimators` and `max_features`,` max_depth` and `min_samples_split` in a `param_dict`\n",
        "2. Create two lists - results and estimators\n",
        "  * `results` will store the `oob_score` for each tree you fit\n",
        "  * `estimators` will score the parameters for each tree\n",
        "3. Run your model through values of the `param_dict`\n",
        "4. Get the best value of oob_score from results, and assign that to `outparams` variable\n",
        "5. Get the best estimator from the `outparams` for the best `oob_score`\n",
        "6. Check the accuracy of the model(best_estimator)\n",
        "7. Plot the confusion matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O9-jnpJVCRa"
      },
      "source": [
        "from itertools import product\n",
        "from collections import OrderedDict\n",
        "#setting the depth and min_samples_split and max_features\n",
        "#your code here\n",
        "param_dict = OrderedDict('n_estimators' = ________, 'max_features' = ________, 'min_samples_split' = _________)\n",
        "param_dict.values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAMN87a7VMg4"
      },
      "source": [
        "results = {}\n",
        "estimators= {}\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGMAMsGtHPmJ"
      },
      "source": [
        "#Call the best estimator from the previous loop of estimators for outparams as Random_Oob\n",
        "#your code here\n",
        "Random_Oob = ___________"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IrsmErYHZIf"
      },
      "source": [
        "#Performance\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I42_8WQ2HfL_"
      },
      "source": [
        "#Plot the confusion matrix \n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76ijoYIePt8D"
      },
      "source": [
        "##### Comparing OOB_score with Random Forest cross validation \n",
        "\n",
        "In the beginning of section 2.4 you found the best parameters to set for your RandomForest Classifer in the **RF_model**.\n",
        "\n",
        "In this step, build a model call it RF_compare, and set the parameters same as the best params you got from **RF_model** and also set the **oob_score as true**. \n",
        "Compare the results you get from this model against the base model you had for Randomforest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIlF_nvfQf3v"
      },
      "source": [
        "#define the model, set the best parameters and oob_score = true\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKSiHl6sQf30"
      },
      "source": [
        "#Performance\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqbbchEkQf30"
      },
      "source": [
        "#plot the confusion matrix\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-sc8Jw4QqE6"
      },
      "source": [
        "###### Does your **RF_compare** model perform better or worse than the cross validated model - **RF_model**? Do you think its necessary to cross validate for a random forest?\n",
        "\n",
        "your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_HB3eZsGrB8"
      },
      "source": [
        "### 2.5 Boosting Tree **(10 marks)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "272B092KreZv"
      },
      "source": [
        "Adaboost Classification, is a special case of a gradient-boosted algorithm. Gradient Boosting is very state of the art, and has major connections to logistic regression, gradient descent in a functional space, and search in information space.\n",
        "\n",
        "The idea is that we will use a bunch of weak 'learners' (aka, models) which are fit sequentially. The first one fits the signal, the second one the first model's residual, the third the second model's residual, and so on. At each stage we upweight the places that our previous model did badly on. \n",
        "\n",
        "Things to remember:\n",
        "* `n_estimators` is the number of trees, and thus the stage in the fitting. It also controls the complexity for us. The more trees we have the more we fit to the tiny details.\n",
        "*`learning_rate` Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsBrhPjCnQjZ"
      },
      "source": [
        "Steps to follow\n",
        "1. Provide the model as AdaBoostclassifer with `random state = rf`\n",
        "2. Set the boosting parameters - boost__n_estimators to be a range from 10 to 120. and `\"boost__learning_rate\":[0.001,0.01,0.1,0.2, 0.5,1,1.2,1.5]`\n",
        "3. Pass the bagging parameters and pipeline to GridSearchCV and set the `cv = 10` and `scoring = 'accuracy'`\n",
        "4. Fit the model\n",
        "5. Find the best hyperparameters, testing and training accuracy\n",
        "6. Plot the confusion matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWGp4mr2bM01"
      },
      "source": [
        "#Build an adaboost pipe\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeQFvSjBbQGg"
      },
      "source": [
        "#Print the best hyperparameters\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOiQYAIMG9mP"
      },
      "source": [
        "#Performance\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvjlS7c9IJa7"
      },
      "source": [
        "#plot the confusion matrix\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFsD6kCSH_wH"
      },
      "source": [
        "### 2.6 XGboost **(10 marks)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BmSOhp5sAq1"
      },
      "source": [
        "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. To read more about XGBoost, click [here](https://xgboost.readthedocs.io/en/latest/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hArj4Pskn-By"
      },
      "source": [
        "Steps to follow\n",
        "\n",
        "1. Set the classifer as XGBClassifer \n",
        "2. Set the xgb_parameters - n_estimators to be a range from [10,20,30,40,50,60,70,80,90,100,120,130,140,150]. and `'learning_rate': [0.001, 0.01, .1,.4, .45, .5, .55, .6]`\n",
        "3. Pass the xgb_parameters to RandomizedSearchCV and set the cv = 10 and `scoring = 'accuracy'` and `n_iter = 100`\n",
        "4. Fit the model\n",
        "5. Find the best hyperparameters, testing and training accuracy\n",
        "6. Plot the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBxOiS5JIFhc"
      },
      "source": [
        "import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bswKQIiTLdc"
      },
      "source": [
        "#assign the XGB_Classifer\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7MI7w7KII9f"
      },
      "source": [
        "#set parameters\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZaXfrigQxC-"
      },
      "source": [
        "#pass the parameters to the model and fit the model\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdSyHngMIJL3"
      },
      "source": [
        "#get the best parameter values\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAV0X4WgJBDo"
      },
      "source": [
        "#Performance\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dev5W7oZIGrt"
      },
      "source": [
        "#plot the confusion matrix \n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fESTBzaYOTFh"
      },
      "source": [
        "## Part 3: Comparing all models **(10 marks)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFIQNiXP3Rrq"
      },
      "source": [
        "Now that we have built all these models lets compare them:\n",
        "\n",
        "1. Plot all the testing accuracies for all the models - Logistic Regression Model, Decision Tree baseline, Decision tree with ideal_alpha, Decision tree with best depth, Bagging model, Random Forest model, Random Forest with oob_score = true, Boosting Model, XGBoost model\n",
        "\n",
        "2. Plot the overfitting accuracies for all the models - train accuracy - test accuracy\n",
        "\n",
        "3. Plot the confusion matrix for the all the models\n",
        "\n",
        "4. Based on these plots draw a conclusion for which is the best and worst performing model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tHSlofsbRVl"
      },
      "source": [
        "#Comparing performance of all models - testing accuracy\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVyZ6t_YxOb6"
      },
      "source": [
        "#overfitting \n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2ijGNCjxP2R"
      },
      "source": [
        "#confusion matrix\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h8ldyvmpnBT"
      },
      "source": [
        "#### **Which is the best and worse model?**\n",
        "\n",
        "your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUNRUcfd4HpG"
      },
      "source": [
        "## Bonus Question **(20 marks)**\n",
        "The dataset provided to you is already preprocessed - all variables are already one-hot encoded. \n",
        "\n",
        "Sklearn requires all its categorical variables to be one-hot encoded. But tree based methods work best with **ordinal encoding**. \n",
        "Get the original data from [here](http://archive.ics.uci.edu/ml/datasets/heart+disease), and ordinal encode the variables. Run one of the tree models to check how ordinal encoding is better. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur5udiu0x0QV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}